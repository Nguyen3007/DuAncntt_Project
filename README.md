# Há»‡ Thá»‘ng Gá»£i Ã Sáº£n Pháº©m - LightGCN & NGCF

## ğŸ“‹ Tá»•ng Quan

Repository nÃ y triá»ƒn khai hai mÃ´ hÃ¬nh Graph Neural Network (GNN) tiÃªn tiáº¿n cho bÃ i toÃ¡n **Collaborative Filtering** (Lá»c cá»™ng tÃ¡c) trong há»‡ thá»‘ng gá»£i Ã½:

1. **LightGCN** - Light Graph Convolutional Network
2. **NGCF** - Neural Graph Collaborative Filtering

Cáº£ hai mÃ´ hÃ¬nh Ä‘á»u Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn dataset **H&M Fashion** vá»›i khoáº£ng **556,884 ngÆ°á»i dÃ¹ng** vÃ  hÃ ng chá»¥c nghÃ¬n sáº£n pháº©m thá»i trang.

## ğŸ¯ Má»¥c ÄÃ­ch Dá»± Ãn

Dá»± Ã¡n nÃ y giáº£i quyáº¿t bÃ i toÃ¡n **gá»£i Ã½ sáº£n pháº©m thá»i trang** cho ngÆ°á»i dÃ¹ng dá»±a trÃªn lá»‹ch sá»­ mua hÃ ng cá»§a há». Há»‡ thá»‘ng sá»­ dá»¥ng:
- **Implicit Feedback**: Chá»‰ cáº§n biáº¿t ngÆ°á»i dÃ¹ng Ä‘Ã£ mua sáº£n pháº©m nÃ o (khÃ´ng cáº§n rating)
- **Graph-based Learning**: Biá»ƒu diá»…n quan há»‡ user-item dÆ°á»›i dáº¡ng Ä‘á»“ thá»‹ Ä‘á»ƒ há»c embeddings tá»‘t hÆ¡n
- **BPR Loss**: Bayesian Personalized Ranking Ä‘á»ƒ tá»‘i Æ°u hÃ³a thá»© tá»± sáº£n pháº©m

## ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
DuAncntt_Project/
â”œâ”€â”€ src/                          # MÃ£ nguá»“n chÃ­nh
â”‚   â”œâ”€â”€ models/                   # CÃ¡c mÃ´ hÃ¬nh GNN
â”‚   â”‚   â”œâ”€â”€ LightGCN.py          # MÃ´ hÃ¬nh LightGCN
â”‚   â”‚   â””â”€â”€ NGCF.py              # MÃ´ hÃ¬nh NGCF
â”‚   â””â”€â”€ data_utils/               # CÃ´ng cá»¥ xá»­ lÃ½ dá»¯ liá»‡u
â”‚       â”œâ”€â”€ dataloader.py         # Load dá»¯ liá»‡u tá»« file txt
â”‚       â”œâ”€â”€ graph_builder.py      # XÃ¢y dá»±ng Ä‘á»“ thá»‹ binary
â”‚       â””â”€â”€ graph_builder_time_decay.py  # Äá»“ thá»‹ vá»›i time-decay weights
â”œâ”€â”€ data/                         # ThÆ° má»¥c dá»¯ liá»‡u
â”‚   â””â”€â”€ h_m/                      # Dataset H&M
â”‚       â”œâ”€â”€ train.txt             # Dá»¯ liá»‡u training
â”‚       â”œâ”€â”€ val.txt               # Dá»¯ liá»‡u validation
â”‚       â”œâ”€â”€ test.txt              # Dá»¯ liá»‡u test
â”‚       â”œâ”€â”€ split_manifest.json   # ThÃ´ng tin vá» split
â”‚       â”œâ”€â”€ user_id_map_9m.csv    # Mapping user IDs
â”‚       â””â”€â”€ item_id_map_9m.csv    # Mapping item IDs
â”œâ”€â”€ train_lightGCN_v2.py         # Script training LightGCN
â”œâ”€â”€ train_ngcf.py                 # Script training NGCF
â”œâ”€â”€ evaluate_lightgcn.py          # Script Ä‘Ã¡nh giÃ¡ LightGCN
â”œâ”€â”€ evaluate_ngcf.py              # Script Ä‘Ã¡nh giÃ¡ NGCF
â””â”€â”€ requirements.txt              # Dependencies

```

## ğŸ” Chi Tiáº¿t Vá» Dá»¯ Liá»‡u

### Format Dá»¯ Liá»‡u

Dá»¯ liá»‡u Ä‘Æ°á»£c tá»• chá»©c theo format **LightGCN-style**:

**train.txt**: Má»—i dÃ²ng chá»©a lá»‹ch sá»­ mua hÃ ng cá»§a 1 user
```
user_id item1 item2 item3 ... itemN
```
VÃ­ dá»¥:
```
0 20556 5085 132 17949 5009 12202 1001 17453 ...
1 1 2 3 84 22500 16255 4020 26867 ...
```

**val.txt** vÃ  **test.txt**: Má»—i user cÃ³ Ä‘Ãºng 1 item (leave-last-1 strategy)
```
user_id item_id
```
VÃ­ dá»¥:
```
0 41197
1 42332
2 33118
```

### Thá»‘ng KÃª Dataset H&M

- **Sá»‘ lÆ°á»£ng users**: 556,884
- **Sá»‘ lÆ°á»£ng items**: ~40,000+ sáº£n pháº©m thá»i trang
- **Split strategy**: Temporal split (theo thá»i gian)
  - Training: Táº¥t cáº£ items (unique) cá»§a má»—i user (trá»« 2 items cuá»‘i)
  - Validation: Item cuá»‘i cÃ¹ng thá»© 2
  - Test: Item cuá»‘i cÃ¹ng

## ğŸ§  Giáº£i ThÃ­ch CÃ¡c MÃ´ HÃ¬nh

### 1. LightGCN (Light Graph Convolutional Network)

**Ã tÆ°á»Ÿng chÃ­nh**: ÄÆ¡n giáº£n hÃ³a GCN cho Collaborative Filtering báº±ng cÃ¡ch loáº¡i bá» cÃ¡c thÃ nh pháº§n khÃ´ng cáº§n thiáº¿t.

**Kiáº¿n trÃºc**:
```
1. Embedding Layer: Khá»Ÿi táº¡o vector cho má»—i user vÃ  item
2. Graph Propagation: 
   - Layer 0: Eâ½â°â¾ = Embedding gá»‘c
   - Layer k: Eâ½áµâ¾ = Ã‚ Ã— Eâ½áµâ»Â¹â¾  (chá»‰ lÃ  matrix multiplication)
   - Ã‚: Normalized adjacency matrix
3. Layer Combination:
   Final_Embedding = mean(Eâ½â°â¾, Eâ½Â¹â¾, ..., Eâ½á´·â¾)
```

**Äáº·c Ä‘iá»ƒm**:
- âœ… Ráº¥t Ä‘Æ¡n giáº£n, khÃ´ng cÃ³ activation function hay transformation matrix
- âœ… Hiá»‡u quáº£ vá» máº·t tÃ­nh toÃ¡n
- âœ… State-of-the-art performance trÃªn nhiá»u datasets
- âš™ï¸ KhÃ´ng sá»­ dá»¥ng self-loop (A, khÃ´ng pháº£i A+I)

**Code chÃ­nh** (`src/models/LightGCN.py`):
```python
def propagate(self, adj: torch.Tensor) -> torch.Tensor:
    x = self.embedding.weight  # Eâ½â°â¾
    embs = [x]
    
    for _ in range(self.n_layers):
        x = torch.sparse.mm(adj, x)  # Eâ½áµâ¾ = Ã‚ Ã— Eâ½áµâ»Â¹â¾
        embs.append(x)
    
    all_embeddings = torch.stack(embs, dim=0).mean(dim=0)
    return all_embeddings
```

### 2. NGCF (Neural Graph Collaborative Filtering)

**Ã tÆ°á»Ÿng chÃ­nh**: MÃ´ hÃ¬nh hÃ³a high-order connectivity báº±ng cÃ¡ch há»c message passing phá»©c táº¡p hÆ¡n.

**Kiáº¿n trÃºc**:
```
1. Embedding Layer: Vector ban Ä‘áº§u cho user/item
2. Message Passing (má»—i layer k):
   a) Graph Convolution:
      msgâ‚ = LeakyReLU((Ã‚ Ã— Eâ½áµâ»Â¹â¾) Ã— Wâ‚ + bâ‚)
   
   b) Bi-Interaction:
      msgâ‚‚ = LeakyReLU((Eâ½áµâ»Â¹â¾ âŠ™ Ã‚Ã—Eâ½áµâ»Â¹â¾) Ã— Wâ‚‚ + bâ‚‚)
   
   c) Combination:
      Eâ½áµâ¾ = msgâ‚ + msgâ‚‚
      
3. Final Embedding: Concat táº¥t cáº£ layers
   Final = [Eâ½â°â¾ || Eâ½Â¹â¾ || ... || Eâ½á´·â¾]
```

**Äáº·c Ä‘iá»ƒm**:
- âœ… Há»c Ä‘Æ°á»£c tÆ°Æ¡ng tÃ¡c phá»©c táº¡p hÆ¡n giá»¯a users vÃ  items
- âœ… Bi-interaction term giÃºp capture feature interactions
- âš™ï¸ Sá»­ dá»¥ng self-loop (A+I) trong adjacency matrix
- âš™ï¸ Message dropout Ä‘á»ƒ regularization

**Code chÃ­nh** (`src/models/NGCF.py`):
```python
def _propagate_impl(self, adj: torch.Tensor) -> torch.Tensor:
    ego_embeddings = self.embedding.weight
    all_embeddings = [ego_embeddings]
    
    x = ego_embeddings
    for k in range(self.n_layers):
        # Graph convolution
        side_embeddings = torch.sparse.mm(adj, x)
        sum_embeddings = torch.matmul(side_embeddings, self.W_gc[k]) + self.b_gc[k]
        sum_embeddings = F.leaky_relu(sum_embeddings)
        
        # Bi-interaction
        bi = x * side_embeddings
        bi_embeddings = torch.matmul(bi, self.W_bi[k]) + self.b_bi[k]
        bi_embeddings = F.leaky_relu(bi_embeddings)
        
        # Combine
        x = sum_embeddings + bi_embeddings
        x = F.dropout(x, p=self.mess_dropout, training=self.training)
        
        all_embeddings.append(x)
    
    return torch.cat(all_embeddings, dim=1)
```

## ğŸ“ BPR Loss (Bayesian Personalized Ranking)

Cáº£ hai mÃ´ hÃ¬nh Ä‘á»u sá»­ dá»¥ng **BPR Loss** Ä‘á»ƒ training:

```python
# Cho má»—i user u:
#   - pos_item: item mÃ  user Ä‘Ã£ tÆ°Æ¡ng tÃ¡c
#   - neg_item: item mÃ  user chÆ°a tÆ°Æ¡ng tÃ¡c (random sampling)

score_pos = user_emb Â· pos_item_emb
score_neg = user_emb Â· neg_item_emb

BPR_Loss = -mean(log(sigmoid(score_pos - score_neg)))
L2_Reg = ||embeddings||Â²

Total_Loss = BPR_Loss + Î» Ã— L2_Reg
```

**Intuition**: Model há»c sao cho Ä‘iá»ƒm sá»‘ cá»§a positive items luÃ´n cao hÆ¡n negative items.

## âš™ï¸ XÃ¢y Dá»±ng Äá»“ Thá»‹ (Graph Construction)

### 1. Binary Graph (GraphBuilder)

Äá»“ thá»‹ khÃ´ng trá»ng sá»‘: edge weight = 1 náº¿u user tÆ°Æ¡ng tÃ¡c vá»›i item.

```python
gb = GraphBuilder(
    num_users=num_users,
    num_items=num_items,
    train_user_items=train_dict,
    add_self_loop=False  # LightGCN: False, NGCF: True
)
adj = gb.build_normalized_adj(device='cuda')
```

**Normalized Adjacency**: Ã‚ = D^(-1/2) Ã— A Ã— D^(-1/2)

### 2. Time-Decay Graph (TimeDecayGraphBuilder) ğŸ”¥

Äá»“ thá»‹ cÃ³ trá»ng sá»‘ dá»±a trÃªn thá»i gian - **TÃ­nh nÄƒng nÃ¢ng cao**!

**Ã tÆ°á»Ÿng**: Items mua gáº§n Ä‘Ã¢y cÃ³ trá»ng sá»‘ cao hÆ¡n (quan trá»ng hÆ¡n).

```python
gb = TimeDecayGraphBuilder(
    num_users=num_users,
    num_items=num_items,
    weight_csv='data/h_m/train_time_weights.csv',
    add_self_loop=False
)
adj = gb.build_normalized_adj(device='cuda')
```

**CÃ´ng thá»©c time-decay**: 
```
weight = exp(-Î± Ã— Î”t)
```
Trong Ä‘Ã³:
- Î”t: khoáº£ng thá»i gian tá»« thá»i Ä‘iá»ƒm tÆ°Æ¡ng tÃ¡c Ä‘áº¿n hiá»‡n táº¡i
- Î±: decay rate (thÆ°á»ng lÃ  0.001 - 0.1)

## ğŸš€ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### CÃ i Äáº·t Dependencies

```bash
pip install -r requirements.txt
```

Requirements:
- torch
- numpy
- pandas

### 1. Training LightGCN

**Binary Graph (Ä‘Æ¡n giáº£n)**:
```bash
python train_lightGCN_v2.py \
    --data_dir data/h_m \
    --emb_dim 64 \
    --n_layers 2 \
    --lr 5e-4 \
    --batch_size 16384 \
    --epochs 30 \
    --steps_per_epoch 800 \
    --device cuda \
    --checkpoint_dir checkpoints \
    --early_stop_patience 3
```

**Time-Decay Graph (nÃ¢ng cao)**:
```bash
python train_lightGCN_v2.py \
    --data_dir data/h_m \
    --use_time_decay \
    --time_weight_csv data/h_m/train_time_weights.csv \
    --emb_dim 64 \
    --n_layers 2 \
    --lr 5e-4 \
    --batch_size 16384 \
    --epochs 30 \
    --device cuda
```

### 2. Training NGCF

**Binary Graph**:
```bash
python train_ngcf.py \
    --data_dir data/h_m \
    --emb_dim 64 \
    --layer_sizes 64 64 \
    --lr 1e-3 \
    --batch_size 4096 \
    --epochs 20 \
    --steps_per_epoch 800 \
    --device cuda \
    --mess_dropout 0.1 \
    --early_stop_patience 5
```

**Time-Decay Graph**:
```bash
python train_ngcf.py \
    --data_dir data/h_m \
    --use_time_decay \
    --emb_dim 64 \
    --layer_sizes 64 64 \
    --lr 1e-3 \
    --batch_size 4096 \
    --epochs 20 \
    --device cuda
```

### 3. Evaluation

**Evaluate LightGCN**:
```bash
# TrÃªn validation set
python evaluate_lightgcn.py \
    --data_dir data/h_m \
    --checkpoint checkpoints/lightgcn_hm_best.pt \
    --split val \
    --K 20 \
    --device cuda

# TrÃªn test set
python evaluate_lightgcn.py \
    --data_dir data/h_m \
    --checkpoint checkpoints/lightgcn_hm_best.pt \
    --split test \
    --K 20 \
    --device cuda
```

**Evaluate NGCF**:
```bash
python evaluate_ngcf.py \
    --data_dir data/h_m \
    --checkpoint checkpoints/ngcf_hm_best.pt \
    --split test \
    --K 20 \
    --device cuda
```

## ğŸ“Š Metrics ÄÃ¡nh GiÃ¡

Há»‡ thá»‘ng sá»­ dá»¥ng cÃ¡c metrics chuáº©n cho Recommendation Systems:

1. **Precision@K**: Tá»· lá»‡ items liÃªn quan trong top-K
   ```
   Precision@K = (sá»‘ items Ä‘Ãºng trong top-K) / K
   ```

2. **Recall@K**: Tá»· lá»‡ items liÃªn quan Ä‘Æ°á»£c tÃ¬m tháº¥y
   ```
   Recall@K = (sá»‘ items Ä‘Ãºng trong top-K) / (tá»•ng sá»‘ items Ä‘Ãºng)
   ```

3. **HitRate@K**: Tá»· lá»‡ users cÃ³ Ã­t nháº¥t 1 item Ä‘Ãºng trong top-K
   ```
   HitRate@K = (sá»‘ users cÃ³ hit) / (tá»•ng sá»‘ users)
   ```

4. **NDCG@K** (Normalized Discounted Cumulative Gain): Xem xÃ©t thá»© tá»± ranking
   ```
   NDCG@K = DCG@K / IDCG@K
   ```

5. **MAP@K** (Mean Average Precision): Trung bÃ¬nh precision táº¡i má»i vá»‹ trÃ­ cÃ³ item Ä‘Ãºng

## ğŸ¯ Quy TrÃ¬nh Training

### Early Stopping Strategy

Training sá»­ dá»¥ng **early stopping** dá»±a trÃªn Recall@K trÃªn validation set:

```python
# Pseudo-code
best_recall = 0
patience_counter = 0
max_patience = 3

for epoch in epochs:
    train_one_epoch()
    
    metrics_val = evaluate_on_validation()
    current_recall = metrics_val['recall']
    
    if current_recall > best_recall:
        best_recall = current_recall
        save_best_model()
        patience_counter = 0
    else:
        patience_counter += 1
    
    if patience_counter >= max_patience:
        print("Early stopping!")
        break
```

### Negative Sampling

Má»—i batch training:
```python
for each user in batch:
    - Chá»n 1 positive item (Ä‘Ã£ tÆ°Æ¡ng tÃ¡c)
    - Chá»n 1 negative item (chÆ°a tÆ°Æ¡ng tÃ¡c, random)
    - TÃ­nh BPR loss
```

## ğŸ’¡ CÃ¡c TÃ­nh NÄƒng NÃ¢ng Cao

### 1. Time-Decay Weighting ğŸ•

Tá»± Ä‘á»™ng Ä‘á»•i tÃªn checkpoint khi dÃ¹ng time-decay:
- `lightgcn_hm_best.pt` â†’ `lightgcn_hm_best_td.pt`
- `ngcf_hm_best.pt` â†’ `ngcf_hm_best_td.pt`

### 2. Gradient Clipping

TrÃ¡nh gradient explosion:
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
```

### 3. Checkpoint Management

LÆ°u 2 loáº¡i checkpoints:
- **Best model**: Model tá»‘t nháº¥t theo validation recall
- **Last model**: Model á»Ÿ epoch cuá»‘i (cÃ³ thá»ƒ resume training)

## ğŸ“ˆ Káº¿t Quáº£ Mong Äá»£i

Vá»›i dataset H&M vÃ  cáº¥u hÃ¬nh máº·c Ä‘á»‹nh:

**LightGCN** (sau ~10-15 epochs):
- Recall@20: ~0.05 - 0.08
- NDCG@20: ~0.03 - 0.05

**NGCF** (sau ~15-20 epochs):
- Recall@20: ~0.04 - 0.07
- NDCG@20: ~0.025 - 0.045

*LÆ°u Ã½*: Káº¿t quáº£ cÃ³ thá»ƒ thay Ä‘á»•i tÃ¹y hyperparameters vÃ  random seed.

## ğŸ”§ Hyperparameters Quan Trá»ng

### LightGCN
- `emb_dim`: KÃ­ch thÆ°á»›c embedding vector (32, 64, 128)
- `n_layers`: Sá»‘ layers GCN (1-4, thÆ°á»ng lÃ  2-3)
- `lr`: Learning rate (1e-4 Ä‘áº¿n 1e-3)
- `reg_weight`: L2 regularization (1e-5 Ä‘áº¿n 1e-3)
- `batch_size`: Batch size (4096 - 32768)

### NGCF
- `emb_dim`: KÃ­ch thÆ°á»›c embedding ban Ä‘áº§u
- `layer_sizes`: KÃ­ch thÆ°á»›c cho má»—i layer message passing
- `mess_dropout`: Dropout rate (0.0 - 0.3)
- `leaky_relu_slope`: Slope cho LeakyReLU (0.1 - 0.3)

## ğŸ› Troubleshooting

### CUDA Out of Memory
```bash
# Giáº£m batch_size
python train_lightGCN_v2.py --batch_size 8192 ...

# Hoáº·c dÃ¹ng CPU
python train_lightGCN_v2.py --device cpu ...
```

### Training quÃ¡ cháº­m
```bash
# Giáº£m steps_per_epoch
python train_lightGCN_v2.py --steps_per_epoch 400 ...

# Giáº£m sá»‘ epochs (dÃ¹ng early stopping)
python train_lightGCN_v2.py --epochs 20 --early_stop_patience 3 ...
```

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

### Papers

1. **LightGCN**: He et al. "LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation" (SIGIR 2020)
   - Paper: https://arxiv.org/abs/2002.02126

2. **NGCF**: Wang et al. "Neural Graph Collaborative Filtering" (SIGIR 2019)
   - Paper: https://arxiv.org/abs/1905.08108

3. **BPR**: Rendle et al. "BPR: Bayesian Personalized Ranking from Implicit Feedback" (UAI 2009)

### Code References
- Original LightGCN: https://github.com/gusye1234/LightGCN-PyTorch
- Original NGCF: https://github.com/xiangwang1223/neural_graph_collaborative_filtering

## ğŸ¤ ÄÃ³ng GÃ³p

Náº¿u báº¡n muá»‘n Ä‘Ã³ng gÃ³p cho dá»± Ã¡n:
1. Fork repository
2. Táº¡o branch má»›i (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Táº¡o Pull Request

## ğŸ“ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  nghiÃªn cá»©u.

## ğŸ‘¨â€ğŸ’» TÃ¡c Giáº£

Repository Ä‘Æ°á»£c táº¡o bá»Ÿi Nguyen3007

## ğŸ“ LiÃªn Há»‡

Náº¿u cÃ³ cÃ¢u há»i hoáº·c váº¥n Ä‘á», vui lÃ²ng táº¡o issue trÃªn GitHub.

---

**Happy Coding! ğŸš€**
