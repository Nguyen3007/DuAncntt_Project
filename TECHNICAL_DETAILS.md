# Chi Ti·∫øt K·ªπ Thu·∫≠t - Technical Deep Dive

## üìê Ki·∫øn Tr√∫c H·ªá Th·ªëng

### 1. T·ªïng Quan Lu·ªìng D·ªØ Li·ªáu

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Raw Data       ‚îÇ
‚îÇ  (train.txt)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TxtCFDataLoader        ‚îÇ
‚îÇ  - Parse user-item data ‚îÇ
‚îÇ  - Create mappings      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GraphBuilder           ‚îÇ
‚îÇ  - Build adjacency      ‚îÇ
‚îÇ  - Normalize: √Ç = D^¬ΩAD^¬Ω‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Model (LightGCN/NGCF)  ‚îÇ
‚îÇ  - Propagate embeddings ‚îÇ
‚îÇ  - Compute BPR loss     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Optimizer              ‚îÇ
‚îÇ  - Adam optimizer       ‚îÇ
‚îÇ  - Gradient clipping    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Evaluation             ‚îÇ
‚îÇ  - Top-K ranking        ‚îÇ
‚îÇ  - Metrics calculation  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üî¨ Ph√¢n T√≠ch Chi Ti·∫øt DataLoader

### TxtCFDataLoader (`src/data_utils/dataloader.py`)

**Nhi·ªám v·ª• ch√≠nh**:
1. Load 3 file: train.txt, val.txt, test.txt
2. T·ª± ƒë·ªông infer num_users v√† num_items
3. Cung c·∫•p API thu·∫≠n ti·ªán cho model

**C·∫•u tr√∫c d·ªØ li·ªáu n·ªôi b·ªô**:
```python
self.train = {
    0: [20556, 5085, 132, ...],  # user 0 ƒë√£ mua c√°c items n√†y
    1: [1, 2, 3, 84, ...],
    ...
}

self.val = {
    0: [41197],  # user 0 c√≥ 1 item validation
    1: [42332],
    ...
}

self.test = {
    0: [1233],   # user 0 c√≥ 1 item test
    1: [43218],
    ...
}
```

**API Methods**:
- `get_train_pos()`: Tr·∫£ v·ªÅ dict user ‚Üí items cho training
- `get_val_truth()`: Tr·∫£ v·ªÅ dict user ‚Üí single_item cho validation
- `get_test_truth()`: Tr·∫£ v·ªÅ dict user ‚Üí single_item cho test

**L∆∞u √Ω quan tr·ªçng**:
- Validation v√† test ch·ªâ c√≥ 1 item/user (leave-last-1 strategy)
- Training c√≥ th·ªÉ c√≥ nhi·ªÅu items/user
- User IDs v√† Item IDs b·∫Øt ƒë·∫ßu t·ª´ 0

## üåê Graph Construction - Chi Ti·∫øt

### Binary GraphBuilder

**Input**: Dictionary `train_user_items = {user: [items]}`

**Qu√° tr√¨nh x√¢y d·ª±ng ƒë·ªì th·ªã**:

```python
# B∆∞·ªõc 1: T·∫°o bipartite graph (ƒë·ªì th·ªã hai ph√≠a)
# Nodes: [0...U-1] = users, [U...U+I-1] = items

for user in users:
    for item in user's items:
        add_edge(user, item + num_users)  # user ‚Üí item
        add_edge(item + num_users, user)  # item ‚Üí user (undirected)

# B∆∞·ªõc 2: T√≠nh degree c·ªßa m·ªói node
degree[i] = s·ªë edges k·ªÅ v·ªõi node i

# B∆∞·ªõc 3: Symmetric normalization
for each edge (i, j) with weight w_ij:
    normalized_weight = w_ij / sqrt(degree[i] * degree[j])

# B∆∞·ªõc 4: (Optional) Add self-loop cho NGCF
if add_self_loop:
    for each node i:
        add_edge(i, i, weight=1.0)
```

**Ma tr·∫≠n Adjacency chu·∫©n h√≥a**:

C√¥ng th·ª©c: √Ç = D^(-1/2) √ó A √ó D^(-1/2)

V√≠ d·ª• v·ªõi ƒë·ªì th·ªã nh·ªè:
```
Users: [0, 1]
Items: [0, 1, 2]  ‚Üí Nodes: [2, 3, 4] (offset by num_users=2)

Interactions:
- User 0 bought items [0, 1]
- User 1 bought items [1, 2]

Adjacency matrix A (binary):
     0  1  2  3  4
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
0 ‚îÇ 0  0  1  1  0  ‚îÇ  user 0
1 ‚îÇ 0  0  0  1  1  ‚îÇ  user 1
2 ‚îÇ 1  0  0  0  0  ‚îÇ  item 0
3 ‚îÇ 1  1  0  0  0  ‚îÇ  item 1
4 ‚îÇ 0  1  0  0  0  ‚îÇ  item 2
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Degree matrix D:
D = diag([2, 2, 1, 2, 1])  # s·ªë edges c·ªßa m·ªói node

Normalized √Ç:
     0      1      2      3      4
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
0 ‚îÇ 0      0     1/‚àö2  1/2    0     ‚îÇ
1 ‚îÇ 0      0      0    1/2   1/‚àö2   ‚îÇ
2 ‚îÇ 1/‚àö2   0      0     0     0     ‚îÇ
3 ‚îÇ 1/2   1/2     0     0     0     ‚îÇ
4 ‚îÇ 0    1/‚àö2     0     0     0     ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Time-Decay GraphBuilder

**ƒêi·ªÉm kh√°c bi·ªát**: Tr·ªçng s·ªë edges kh√¥ng ph·∫£i l√† 1, m√† ph·ª• thu·ªôc th·ªùi gian

**Input CSV** (`train_time_weights.csv`):
```csv
u,v,weight
0,20556,0.9523
0,5085,0.8891
0,132,0.7234
...
```

**C√¥ng th·ª©c time-decay**:
```python
# Gi·∫£ s·ª≠ c√≥ timestamp t_interaction
delta_t = t_current - t_interaction
weight = exp(-decay_rate * delta_t)

# V√≠ d·ª•:
# Item mua 1 ng√†y tr∆∞·ªõc: weight ‚âà 0.95
# Item mua 30 ng√†y tr∆∞·ªõc: weight ‚âà 0.74
# Item mua 90 ng√†y tr∆∞·ªõc: weight ‚âà 0.41
```

**Normalization v·ªõi weights**:
```python
# Degree with weights
degree[i] = sum(weight_ij for all j connected to i)

# Symmetric normalization
normalized_weight_ij = weight_ij / sqrt(degree[i] * degree[j])
```

## üß© Ki·∫øn Tr√∫c Model Chi Ti·∫øt

### LightGCN Architecture

**Layer-by-layer breakdown**:

```python
# Initialization
E_0 = Embedding_matrix  # Shape: [num_users + num_items, emb_dim]

# Layer 1
E_1 = √Ç @ E_0
# √ù nghƒ©a: M·ªói node nh·∫≠n trung b√¨nh weighted embedding t·ª´ neighbors

# Layer 2
E_2 = √Ç @ E_1 = √Ç @ (√Ç @ E_0) = √Ç¬≤ @ E_0
# √ù nghƒ©a: M·ªói node nh·∫≠n th√¥ng tin t·ª´ 2-hop neighbors

# Layer K
E_K = √Ç^K @ E_0
# √ù nghƒ©a: Th√¥ng tin t·ª´ K-hop neighbors

# Final Embedding
E_final = (E_0 + E_1 + E_2 + ... + E_K) / (K + 1)
```

**T·∫°i sao kh√¥ng c·∫ßn activation function?**
- Paper ch·ª©ng minh r·∫±ng cho Collaborative Filtering, linear propagation ƒë·ªß t·ªët
- Activation functions (ReLU, sigmoid) c√≥ th·ªÉ l√†m m·∫•t information
- ƒê∆°n gi·∫£n h∆°n = √≠t overfitting h∆°n

**Complexity Analysis**:
```
Time: O(K √ó |E| √ó d)  # K layers, |E| edges, d embedding dim
Space: O((U + I) √ó d)  # U users, I items
```

### NGCF Architecture

**Detailed Message Passing**:

```python
# Layer k message passing
for each layer k:
    # 1. Aggregate neighbors
    neighbor_sum = √Ç @ E_{k-1}  # Shape: [N, d]
    
    # 2. First-order propagation (Graph Convolution)
    msg_1 = LeakyReLU(neighbor_sum @ W_gc[k] + b_gc[k])
    # Transformation matrix W_gc h·ªçc c√°ch combine neighbor info
    
    # 3. Second-order propagation (Bi-Interaction)
    # Element-wise product: capture pairwise feature interactions
    bi_interaction = E_{k-1} ‚äô neighbor_sum
    msg_2 = LeakyReLU(bi_interaction @ W_bi[k] + b_bi[k])
    
    # 4. Combine messages
    E_k = msg_1 + msg_2
    
    # 5. Dropout (regularization)
    E_k = Dropout(E_k, p=mess_dropout)

# Final: Concatenate all layers
E_final = [E_0 || E_1 || E_2 || ... || E_K]
# Shape: [N, (K+1) √ó d]
```

**Bi-Interaction Term - T·∫°i sao quan tr·ªçng?**

V√≠ d·ª• c·ª• th·ªÉ:
```
User u c√≥ embedding: [0.5, 0.3, 0.8]
Item i c√≥ embedding:  [0.2, 0.7, 0.4]

# Kh√¥ng c√≥ bi-interaction:
Combined = W √ó (u + i) = linear combination

# C√≥ bi-interaction:
Bi = u ‚äô i = [0.1, 0.21, 0.32]  # element-wise product
Combined = W1 √ó (u + i) + W2 √ó Bi
# Capture ƒë∆∞·ª£c t∆∞∆°ng t√°c gi·ªØa c√°c features
```

**Complexity Analysis**:
```
Time: O(K √ó |E| √ó d + K √ó d¬≤)  # Matrix multiplications
Space: O((U + I) √ó (K+1) √ó d)  # L∆∞u t·∫•t c·∫£ layers
```

## üéØ BPR Loss - Mathematical Derivation

### Bayesian Personalized Ranking

**Intuition**: Minimize pairwise ranking loss

**Formulation**:

```
Given:
- User u
- Positive item i (user interacted)
- Negative item j (user NOT interacted)

Goal: Score(u, i) > Score(u, j)

Likelihood:
P(i >_u j | Œò) = œÉ(x_uij)

where:
x_uij = score(u, i) - score(u, j)
œÉ = sigmoid function

Maximum Likelihood:
max_Œò ‚àè_{u,i,j} œÉ(x_uij)

Log-likelihood:
max_Œò ‚àë_{u,i,j} log(œÉ(x_uij))

Minimize negative log-likelihood:
min_Œò -‚àë_{u,i,j} log(œÉ(x_uij))

With L2 regularization:
Loss = -‚àë_{u,i,j} log(œÉ(x_uij)) + Œª||Œò||¬≤
```

### Code Implementation

```python
def bpr_loss(user_emb, pos_emb, neg_emb, reg_weight):
    # Shape: [batch_size, emb_dim]
    
    # Scores
    pos_scores = (user_emb * pos_emb).sum(dim=1)  # [B]
    neg_scores = (user_emb * neg_emb).sum(dim=1)  # [B]
    
    # BPR: -mean(log(sigmoid(pos - neg)))
    diff = pos_scores - neg_scores
    bpr = -F.logsigmoid(diff).mean()
    
    # L2 regularization on ego embeddings
    # (only on initial embeddings, not propagated ones)
    reg = (user_emb.norm(2).pow(2) + 
           pos_emb.norm(2).pow(2) + 
           neg_emb.norm(2).pow(2)) / batch_size
    
    loss = bpr + reg_weight * reg
    return loss
```

### Negative Sampling Strategy

**Uniform Sampling**:
```python
def sample_negative(user, num_items, user_positive_set):
    while True:
        neg_item = random.randint(0, num_items - 1)
        if neg_item not in user_positive_set:
            return neg_item
```

**T·∫°i sao uniform sampling?**
- ƒê∆°n gi·∫£n, hi·ªáu qu·∫£
- Paper ch·ª©ng minh t·ªët h∆°n popularity-based sampling
- Tr√°nh model bias v·ªÅ popular items

## üìä Evaluation Metrics - Detailed Calculation

### Recall@K

```python
def recall_at_k(recommended_items, ground_truth_items, K):
    """
    recommended_items: Top-K items
    ground_truth_items: Items user actually interacted with
    """
    hits = len(set(recommended_items[:K]) & set(ground_truth_items))
    recall = hits / len(ground_truth_items)
    return recall

# Example:
recommended = [5, 2, 8, 1, 9]  # Top-5
ground_truth = [2, 7, 9]        # 3 relevant items

hits = {2, 9}  # 2 items in common
recall@5 = 2 / 3 = 0.667
```

### NDCG@K (Normalized Discounted Cumulative Gain)

**Intuition**: Items ranked higher should get more credit

```python
import numpy as np

def ndcg_at_k(recommended, ground_truth, K):
    # DCG: Discounted Cumulative Gain
    dcg = 0.0
    for i, item in enumerate(recommended[:K]):
        if item in ground_truth:
            rank = i + 1  # 1-indexed
            dcg += 1.0 / np.log2(rank + 1)
    
    # IDCG: Ideal DCG (best possible ranking)
    idcg = 0.0
    for i in range(min(len(ground_truth), K)):
        rank = i + 1
        idcg += 1.0 / np.log2(rank + 1)
    
    ndcg = dcg / idcg if idcg > 0 else 0.0
    return ndcg

# Example:
recommended = [5, 2, 8, 1, 9]  # Positions: [1, 2, 3, 4, 5]
ground_truth = [2, 9]

# Item 2 at position 2: 1/log2(3) = 0.631
# Item 9 at position 5: 1/log2(6) = 0.387
DCG = 0.631 + 0.387 = 1.018

# Ideal: both items at positions 1 and 2
# Position 1: 1/log2(2) = 1.0
# Position 2: 1/log2(3) = 0.631
IDCG = 1.0 + 0.631 = 1.631

NDCG@5 = 1.018 / 1.631 = 0.624
```

### MAP@K (Mean Average Precision)

```python
def average_precision_at_k(recommended, ground_truth, K):
    if len(ground_truth) == 0:
        return 0.0
    
    hits = 0
    sum_precisions = 0.0
    
    for i, item in enumerate(recommended[:K]):
        if item in ground_truth:
            hits += 1
            precision_at_i = hits / (i + 1)
            sum_precisions += precision_at_i
    
    ap = sum_precisions / min(len(ground_truth), K)
    return ap

# Example:
recommended = [5, 2, 8, 9, 1]
ground_truth = [2, 9]

# Item 2 at pos 2: precision = 1/2 = 0.5
# Item 9 at pos 4: precision = 2/4 = 0.5
AP@5 = (0.5 + 0.5) / 2 = 0.5
```

## ‚ö° Optimization Techniques

### 1. Sparse Matrix Operations

```python
# PyTorch sparse matrix multiplication
adj_sparse = torch.sparse_coo_tensor(indices, values, size)

# Efficient: O(nnz √ó d) where nnz = number of non-zeros
result = torch.sparse.mm(adj_sparse, embeddings)

# vs Dense (very slow): O(N¬≤ √ó d)
# result = torch.mm(adj_dense, embeddings)
```

### 2. Batch Propagation

```python
# BAD: Propagate for each batch (slow)
for batch in batches:
    user_emb, item_emb = model.propagate(adj, batch)
    loss = compute_loss(user_emb, item_emb)
    
# GOOD: Propagate once for all nodes
all_emb = model.propagate(adj)  # Once per epoch
for batch in batches:
    user_emb = all_emb[batch.users]
    item_emb = all_emb[batch.items]
    loss = compute_loss(user_emb, item_emb)
```

### 3. Gradient Clipping

```python
# Prevent gradient explosion
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)

# Why needed?
# - Deep propagation can cause gradient explosion
# - Especially in early epochs with random initialization
```

### 4. Mixed Precision Training (Future)

```python
# For faster training on modern GPUs
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in batches:
    with autocast():
        loss = model.forward(batch)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

## üîç Debugging Tips

### Ki·ªÉm tra Graph Construction

```python
# Verify adjacency matrix
adj = gb.build_normalized_adj()

print(f"Shape: {adj.shape}")  # Should be [N, N]
print(f"Non-zeros: {adj._nnz()}")  # Number of edges
print(f"Is coalesced: {adj.is_coalesced()}")  # Should be True

# Check symmetry (for undirected graph)
adj_dense = adj.to_dense()
is_symmetric = torch.allclose(adj_dense, adj_dense.t())
print(f"Symmetric: {is_symmetric}")  # Should be True

# Check normalization (row sums should be consistent)
row_sums = torch.sparse.sum(adj, dim=1).to_dense()
print(f"Row sum stats: min={row_sums.min()}, max={row_sums.max()}")
```

### Monitor Training

```python
# Track metrics every epoch
metrics_history = {
    'train_loss': [],
    'val_recall': [],
    'val_ndcg': []
}

for epoch in epochs:
    train_loss = train_epoch()
    val_metrics = evaluate(split='val')
    
    metrics_history['train_loss'].append(train_loss)
    metrics_history['val_recall'].append(val_metrics['recall'])
    metrics_history['val_ndcg'].append(val_metrics['ndcg'])
    
    # Plot or save
    plot_metrics(metrics_history)
```

### Check Embedding Quality

```python
# After training
user_embs, item_embs = model.get_user_item_embeddings(adj)

# Check embedding norm
user_norms = user_embs.norm(dim=1)
print(f"User embedding norms: mean={user_norms.mean():.4f}, "
      f"std={user_norms.std():.4f}")

# Check for NaN/Inf
assert not torch.isnan(user_embs).any()
assert not torch.isinf(user_embs).any()

# Check similarity distribution
similarities = torch.matmul(user_embs, user_embs.t())
print(f"User-user similarity: min={similarities.min():.4f}, "
      f"max={similarities.max():.4f}, "
      f"mean={similarities.mean():.4f}")
```

## üöÄ Performance Tuning

### Memory Optimization

```python
# 1. Use smaller batch size
--batch_size 4096  # instead of 16384

# 2. Reduce embedding dimension
--emb_dim 32  # instead of 64

# 3. Fewer layers
--n_layers 1  # instead of 3

# 4. Use CPU if GPU memory is limited
--device cpu
```

### Speed Optimization

```python
# 1. Increase batch size (if memory allows)
--batch_size 32768

# 2. Reduce steps_per_epoch
--steps_per_epoch 400  # instead of 800

# 3. Use DataLoader with multiple workers (future improvement)
DataLoader(dataset, num_workers=4, pin_memory=True)

# 4. Profile code
import torch.autograd.profiler as profiler

with profiler.profile(use_cuda=True) as prof:
    model.forward(batch)

print(prof.key_averages().table())
```

## üìà Experiment Tracking (Recommended)

### Using TensorBoard

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/lightgcn_experiment_1')

for epoch in epochs:
    # Log training loss
    writer.add_scalar('Loss/train', train_loss, epoch)
    
    # Log validation metrics
    writer.add_scalar('Recall@20/val', val_recall, epoch)
    writer.add_scalar('NDCG@20/val', val_ndcg, epoch)
    
    # Log learning rate
    writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)

writer.close()

# View: tensorboard --logdir=runs
```

### Using Weights & Biases (W&B)

```python
import wandb

wandb.init(project="recommendation-system", 
           config={
               "emb_dim": 64,
               "n_layers": 2,
               "lr": 5e-4,
           })

for epoch in epochs:
    wandb.log({
        "train_loss": train_loss,
        "val_recall": val_recall,
        "val_ndcg": val_ndcg,
        "epoch": epoch
    })
```

## üéì Advanced Topics

### 1. Cold Start Problem

**V·∫•n ƒë·ªÅ**: Users/items m·ªõi kh√¥ng c√≥ interaction history

**Gi·∫£i ph√°p**:
```python
# 1. Use content features (if available)
user_content_emb = encode_user_profile(user_features)
final_emb = graph_emb + content_emb

# 2. Use popularity baseline for new items
if is_new_item:
    return top_popular_items()

# 3. Hybrid approach
score = Œ± √ó graph_score + (1-Œ±) √ó content_score
```

### 2. Temporal Dynamics

**Time-decay implementation**:
```python
# Option 1: Exponential decay
weight = exp(-Œª √ó Œît)

# Option 2: Linear decay
weight = max(0, 1 - Œª √ó Œît)

# Option 3: Logarithmic decay
weight = 1 / (1 + log(1 + Œît))
```

### 3. Multi-behavior Interactions

**V√≠ d·ª•**: view, cart, purchase
```python
# Build separate graphs
A_view = build_graph(view_interactions)
A_cart = build_graph(cart_interactions)
A_purchase = build_graph(purchase_interactions)

# Weighted combination
A_final = w1 √ó A_view + w2 √ó A_cart + w3 √ó A_purchase

# Or: Separate propagation
E_view = propagate(A_view)
E_cart = propagate(A_cart)
E_purchase = propagate(A_purchase)
E_final = concat([E_view, E_cart, E_purchase])
```

---

**Document n√†y cung c·∫•p hi·ªÉu bi·∫øt s√¢u v·ªÅ implementation details. ƒê·ªçc k√®m README.md ƒë·ªÉ c√≥ overview ho√†n ch·ªânh!**
